---
title: "Institutional Characteristics Analysis"
author: "Karen Detter"
desription: "Machine Learning for Social Science Project"
date: "5/25/2023"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
---

```{r}
library(tidyverse)

knitr::opts_chunk$set(echo = TRUE)
```

# Which Institutional Characteristics Can Predict Whether Undergraduate Students Graduate On Time?

## 1) Background

Every fall, thousands of students enroll in undergraduate university programs with the expectation of obtaining a bachelor's degree after four years of study. In reality, less than half of those students will graduate within the traditional four-year time frame. According to the latest figures from the *Education Data Initiative*, just 41% of students meet that goal (Medved, 2022). Because this increase in time to degree attainment points to potential failures in higher education systems, and also equates to inflated debt load on students, much investigation has been done into the reasons for this phenomenon (Marsh, 2014; Moore & Shulock, 2009; Oseguera, 2005). Some research studies have positioned student-centric variables as the determinants of on-time graduation, implying that there is little to be done about the issue on the institutions' end, outside of identifying and monitoring "at-risk" students. Others, however, have identified various institutional characteristics that have an impact on student ability to complete their degree programs in four years.

Recently, researchers have begun applying machine learning models to analyze this important issue. Two meta-analyses of these studies showed that these techniques showed promising results in accurately predicting the factors that have a significant impact on retention and completion variables (Attiya & Shams, 2023; Cardona et al, 2020). A wide variety of methods were employed across the studies included in these reviews; however, most of them used their own institution-level data sets. In fact, both meta-studies identified more expansive data as a target for future research. This project, therefore, seeks to add to the current literature by applying machine learning algorithms to data from a national clearinghouse of university surveys in order to predict which institutional factors influence the amount of students who graduate within four years.

## 2) Exploratory Data Analysis

The data used for the model is from the National Center for Education Statistics' Data Center, a robust database containing thousands of data points self-reported annually by thousands of higher education institutions. Variables were selected according to those identified in the above-mentioned studies as being influential and filtered over US public and private (not-for-profit) 4-year degree-granting institutions. A custom data set was generated for information from the 2018-2019 school year. The resulting data set contains 1,803 observations for thirty variables.

```{r}
library(tidyverse)  
library(readr)  
library(caret)  
library(glmnet)  
library(randomForest)  
library(ranger)  
library(e1071)  
library(mltools) 
library(MASS) 
library(caTools) 
library(class) 
library(corrplot)
library(MLmetrics)
library(mice)
library(vip)
```

```{r}
#load data set 
data <- read_csv("data/IPEDS_data.csv")  
names(data)
```

### a) Description of Variables

All variables (besides identifiers) are either dichotomous (yes/no) or continuous (% and \$) and can be sorted across the following categories:

**Credits Allowed** includes: *MilCr*-credit given for military service, *DualCr*-college credit can be earned in high school, *LifeCr*-credit given for life experience, *APCr*-Advanced Placement credits accepted

**Services Offered** includes: *VetOrg*-veteran student organization available, *ExpHrs*-weekend/evening classes available, *RemedSvcs*-remedial services available, *AcadCouns*-academic counseling available, *Emp*-employment services for students available, *PlSvcs*-placement services for graduates available, *Daycare*- on-campus daycare services offered, *Online*-undergraduate distance learning offered, *Library*-physical library provided, *LibRes*-access to electronic resources provided, *LibStaff*-library staff available for consult, *LibShare*-access to other institutional libraries available, *Libs*-number of library branches, *Books*-as % of library collection, *Serials*-as % of library collection, *Ebooks*-as % of library collection, *DB*-databases as % of library collection, *Digital*-digital media as % of library collection

**Institutional Profile** includes: *ID*-numeric identifier, *Name*, *Select*-% of applicants accepted

**Financial** includes: *Cost*- total \$ tuition & fees, Aid-% of students receiving any form of tuition assistance, AidAvg-average \$ amount of aid received per student, *InstrExp*- per-student \$ expediture on instruction, *SuppExp*- per-student \$ expenditure on student support services, *SvcsExp*- per-student \$ expenditure on other student services

**Dependent Variable**: *OnTimeGrad*- % of students who graduated with a bachelor's degree in 4 years (as reported for 2019)

### b) Transformation of Variables

Because the data set was able to be customized directly from the database, not much wrangling is required. Some variables do, however, need to be transformed. Since this process will not substantively change the data values, it can be done on the full data set.

```{r}
#create binary variables for yes/no vectors  
data <- data %>%   mutate(MilCr = case_when(     MilCr == "Yes" ~ 1,     MilCr == "Implied no" ~ 0))  
data <- data %>%   mutate(VetOrg = case_when(     VetOrg == "Yes" ~ 1,     VetOrg == "Implied no" ~ 0))  
data <- data %>%   mutate(DualCr = case_when(     DualCr == "Yes" ~ 1,     DualCr == "Implied no" ~ 0))  
data <- data %>%   mutate(LifeCr = case_when(     LifeCr == "Yes" ~ 1,     LifeCr == "Implied no" ~ 0))  
data <- data %>%   mutate(APCr = case_when(     APCr == "Yes" ~ 1,     APCr == "Implied no" ~ 0))  
data <- data %>%   mutate(ExpHrs = case_when(     ExpHrs == "Yes" ~ 1,     ExpHrs == "Implied no" ~ 0))  
data <- data %>%   mutate(RemedSvcs = case_when(     RemedSvcs == "Yes" ~ 1,     RemedSvcs == "Implied no" ~ 0))  
data <- data %>%   mutate(AcadCouns = case_when(     AcadCouns == "Yes" ~ 1,     AcadCouns == "Implied no" ~ 0))  
data <- data %>%   mutate(Emp = case_when(     Emp == "Yes" ~ 1,     Emp == "Implied no" ~ 0))  
data <- data %>%   mutate(PlSvcs = case_when(     PlSvcs == "Yes" ~ 1,     PlSvcs == "Implied no" ~ 0))  
data <- data %>%   mutate(Daycare = case_when(     Daycare == "Yes" ~ 1,     Daycare == "Implied no" ~ 0))  
data <- data %>%   mutate(Online = case_when(     Online == "Yes" ~ 1,     Online == "Implied no" ~ 0))  
data <- data %>%   mutate(Library = case_when(     Library == "Yes" ~ 1,     Library == "Implied no" ~ 0))  
data <- data %>%   mutate(LibRes = case_when(     LibRes == "Yes" ~ 1,     LibRes == "Implied no" ~ 0))  
data <- data %>%   mutate(LibStaff = case_when(     LibStaff == "Yes" ~ 1,     LibStaff == "Implied no" ~ 0))  
data <- data %>%   mutate(LibShare = case_when(     LibShare == "Yes" ~ 1,     LibShare == "Implied no" ~ 0))
```

```{r}
#examine cleaned data 
summary(data)
```

An examination of the variable ranges shows a few large outlier values (within the expenditures variables), but the mean, median, and quartile measures indicate that distribution should be reasonably normal. The expenditures variables also have a large amount of *NA* values that will need to be imputed, which theoretically could substantially change the evaluation and applicability of the model. The missing values are likely due to unavailability of data, so imputation is the only option because expenditures are intuitively important variables to be included in the predictive models.

The data is now ready to be split. Because the data set is somewhat small, the training set will be set to 70% and the test set to 30%, to ensure that both sets are sufficiently representative of the data.

```{r}
#split into training and test sets  
set.seed(123) 
idx <- sample(seq(1, 2), size = nrow(data), replace = TRUE, prob = c(.7, .3)) 
training <- data[idx == 1,] 
test <- data[idx == 2,]
```

## 3) Evaluation Metric

Because the variable of interest, the percentage of students who graduate in four years, is continuous, and the problem to be addressed is one of prediction, regression methods will be the most appropriate models. As such, the metric used in k-fold cross validation to compare model performance will be Root Mean Squared Error. RMSE is a common metric for evaluating regression models, and is a readily available calculation in many R packages. It is a simple computation that measures the average difference between predicted and actual values, making it easy to explain.

RMSLE is likely a more appropriate metric for this data set, as it is neither scale-dependent nor sensitive to outliers. However, it is not readily available as a metric in any well-known R packages with cross validation functions, and customizing the metric would require extensive time and effort, whereas RMSE is an acceptable default metric. As such, as a compromise between precision and efficiency, it will be used to compare performance across models after parameter tuning has been completed.

## 4) Data Preprocessing

### a) Impute NA values

There are many missing values in the expenditure variable vectors, so these will be imputed using the multiple imputation method of predictive mean matching.

```{r}
#subset training without character variables  
training <- subset(training, select = -c(ID,Name)) 
```

```{r}
init = mice(training, maxit=0)
meth = init$method
predM=init$predictorMatrix
meth[c("InstrExp")]='pmm'
meth[c("SuppExp")]='pmm'
meth[c("SvcsExp")]='pmm'
set.seed(123)
imputed = mice(training, method=meth, predictorMatrix=predM, m=5)
imputed_training <- complete(imputed)
```

```{r}
sapply(imputed_training, function(x) sum(is.na(x)))
```

Since all of the data vectors have been imputed with predicted estimated values, the new training set is ready to be used for model fitting.

## 5) Fit Models

Based on their performance in previous research studies, lasso, random forest, and support vector regression models will be built. The properties of these three methods make them appropriate for data sets with a large number of variables and potential outliers.

As a first step, relationships among variables are assessed visually.

```{r}
#assess multicollinearity 
c = cor(imputed_training) 
corrplot(c)  
```

It appears that *Cost* & *AidAvg* and *Book*s & *Ebooks* may be collinear, but not necessarily to a great enough degree to remove the variables, as they are likely still important to the model.

### a) Lasso Regression

```{r}
#create outcome variable object
y <- imputed_training$OnTimeGrad

#create predictor variables object
x <- model.matrix(OnTimeGrad~., imputed_training)[,-1]

#find the best value for lambda using cross-validation
set.seed(123)
cv <- cv.glmnet(x,y,alpha = 1)

#fit model
LR <- glmnet(x,y,alpha=1, lambda=cv$lambda.min)
print(LR)
```

```{r}
#k-fold cross validation  

#define training control 
control = trainControl(method = "cv",                      
                       number = 10)  
#train model  
LRmodel <- train(OnTimeGrad ~ ., data = imputed_training, method = "glmnet",    tuneGrid = expand.grid(alpha = 1, lambda = .1779), trControl = control, preProcess = c('scale'))
print(LRmodel)
```

### b) Random Forest Regression

```{r}
#fit model 
set.seed(123)
RFmodel = ranger(
  OnTimeGrad ~ .,
  imputed_training,
  importance = 'permutation',
  num.trees = 1000)

print(RFmodel)
```

```{r}
#k-fold cross validation  

#define training control 
control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3, 
                        search='grid')

tunegrid <- expand.grid(
  mtry = 1:15,
  splitrule = "variance",
  min.node.size = c(10,20)
)

#train model  
RF <- train( OnTimeGrad ~., 
                       data = imputed_training,
                       method = 'ranger',
                       metric = 'RMSE',
                       tuneGrid = tunegrid)
print(RF)
```

### c) Support Vector Regression

```{r}
#fit model with training data  
SVmodel <- svm(OnTimeGrad ~ ., data=imputed_training)
print(SVmodel)
```

```{r}
#cross validation   
set.seed(123)
SV <- train(
  OnTimeGrad ~., data = imputed_training, method = "svmLinear2",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(cost = seq(0, 2, length = 20)),
  preProcess = c("scale")
  )
# Plot model accuracy vs different values of Cost
plot(SV)
```

```{r}
# Print the best tuning parameter C that
# maximizes model accuracy
SV$bestTune
```

## 6) Compare Models

The RMSE values for the models as fitted are 14.85 for lasso regression, 14.87 for random forest regression, and 15.10 for support vector regression. To make a final evaluation of which model is most accurate, the tuned models will be compared on the more appropriate metric of RMSLE.

### a) Evaluate all tuned models on RMSLE with test data set

```{r}
#data preprocessing of test set

init = mice(test, maxit=0)
meth = init$method
predM=init$predictorMatrix
meth[c("InstrExp")]='pmm'
meth[c("SuppExp")]='pmm'
meth[c("SvcsExp")]='pmm'
set.seed(123)
imputed2 = mice(test, method=meth, predictorMatrix=predM, m=5)
imputed_test <- complete(imputed2)
sapply(imputed_test, function(x) sum(is.na(x)))
```

#### i) Lasso Regression

```{r}
#create outcome variable object
y <- imputed_test$OnTimeGrad

#create predictor variables object
x <- model.matrix(OnTimeGrad~., imputed_test)[,-1]

#fit final model
LRFinal <- glmnet(x,y,alpha=1, lambda=.1779)
```

```{r}
#generate predictions from test set
x.test <- model.matrix(OnTimeGrad ~ ., imputed_test) [,-1]
predictions <- LRFinal %>% predict(x.test) %>% as.vector()
predictions <- as.data.frame(predictions)
#replace negative values with 0
predictions$predictions[predictions$predictions < 0] <- 0

#calculate RMSLE
rmsle(predictions$predictions,imputed_test$OnTimeGrad)
```

#### ii)Random Forest Regression

```{r}
#fit final model 
set.seed(123)
RFFinal = ranger(
  OnTimeGrad ~ .,
  imputed_test,
  importance = 'permutation',
  mtry = 5,
  min.node.size = 10,
  num.trees = 1000)

```

```{r}
#generate predictions from test set
predictions2 <- RFFinal %>% predict(imputed_test)
#calculate RMSLE
rmsle(predictions2$predictions, imputed_test$OnTimeGrad)
```

#### iii) Support Vector Regression

```{r}
#fit final model
SVFinal <- svm(OnTimeGrad ~ ., cost = .1052632, data=imputed_test)
```

```{r}
#generate predictions from test set
predictions3 <- SVFinal %>% predict(imputed_test)
predictions3 <- as.data.frame(predictions3)
#calculate RMSLE
rmsle(predictions3$predictions, imputed_test$OnTimeGrad)
```

With an RMSLE of .37, the lasso regression model should be the best fit for predicting the factors that contribute to on-time graduation of university undergraduates. Although the RMSE values of each model were quite close, using the RMSLE metric appears to help differentiate among them, as these values were much further from one another. Lasso regression is also a good choice because, unlike random forest or support vector regression, it is not a "black box" model and is easily explainable mathematically. The lasso model also seems to offer a good compromise between bias and variance, because the flexibility of having a tuning parameter and the fact that it penalizes having a large number of variables minimizes overfitting, while having a good size training set helps minimize underfitting.

```{r}
#calculate variable importance from final lasso model

finaldata <- subset(imputed_test, select = -c(ID,Name)) 

#create outcome variable object
y <- finaldata$OnTimeGrad

#create predictor variables object
x <- model.matrix(OnTimeGrad~., finaldata)[,-1]

#fit final model
LRFinal <- glmnet(x,y,alpha=1, lambda=.1779)

vip(LRFinal)
```

The lasso regression model also allows for variable importance analysis, which is very helpful in this particular project because it shows which particular institutional characteristics have the greatest potential to influence on-time graduation, so universities with limited budgets could use the information to plan new services according to importance.

## 7) Ethical Implications

Since this predictive model is based on *institutional* characteristics, there is little possibility of incurring ethical issues in its use. Student-based models, on the other hand, could be fraught with potential bias or profiling based on misapplied assumptions about specific populations or demographic markers. Using data about institutional traits to predict student success targets in tandem with the input of substantively-informed decision-makers seems like an acceptable compromise between automation and fairness.

## References

Attiya, W. M., & Shams, M. B. (2023). Predicting Student Retention in Higher Education Using Data Mining Techniques: A Literature Review. *2023 International Conference On Cyber Management And Engineering (CyMaEn), Cyber Management And Engineering (CyMaEn), 2023 International Conference On*, 171--177. <https://doi.org/10.1109/CyMaEn57228.2023.10051056>

Cardona, T., Cudney, E. A., Hoerl, R., & Snyder, J. (2020). Data Mining and Machine Learning Retention Models in Higher Education. *Journal of College Student Retention: Research, Theory & Practice*, *25*(1), 51--75. <https://doi.org/10.1177/1521025120964920>

Marsh, G. (2014). Institutional Characteristics and Student Retention in Public 4-Year Colleges and Universities. *Journal of College Student Retention: Research, Theory & Practice*, *16*(1), 127--151. <https://doi.org/10.2190/CS.16.1.g>

Medved, S. (2022, April 27). *Why College Students May Not Graduate in Four Years---The Bottom Line UCSB*. <https://thebottomline.as.ucsb.edu/2022/04/why-college-students-may-not-graduate-in-four-years>

Moore, C., & Shulock, N. (2009). *Student Progress Toward Degree Completion: Lessons From the Research Literature*. California State University Sacramento Institute for Higher Education Leadership & Policy.

Oseguera, L. (2005). Four and Six-Year Baccalaureate Degree Completion by Institutional Characteristics and Racial/Ethnic Groups. *Journal of College Student Retention: Research, Theory & Practice*, *7*(1/2), 19--59. <https://doi.org/10.2190/E1TU-AW8J-5FYA-GLPW>

U.S. Department of Education. (2018). *National Center for Education Statistics, Integrated Postsecondary Education Data System (IPEDS), \[data set\]*. <https://nces.ed.gov/ipeds/datacenter/InstitutionByName.aspx?goToReportId=1>
